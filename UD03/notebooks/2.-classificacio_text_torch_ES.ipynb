{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "188b48ed5b49e64b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Clasificador de noticias\n",
    "\n",
    "En esta práctica, crearemos un clasificador de noticias utilizando las técnicas de procesamiento del lenguaje natural que hemos visto en clase, centrándose en la representación del texto.\n",
    "\n",
    "Usaremos el `dataset` [AG News](https://www.kaggle.com/amananandrai/ag-news-classification-dataset) que contiene 1,000,000 noticias de 4 categorías diferentes.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Para cargar el conjunto de datos, usaremos la librería `datasets`. Esta librería nos permitirá cargar muchos conjuntos de datos diferentes de una manera simple.En este caso, cargaremos el conjunto de datos AG News."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a731df9a4a91fd19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Preparación del dataset\n",
    "\n",
    "Para instalar las librerías necesarias, ejecutaremos la siguiente celda.\n",
    "\n",
    "Usaremos`pytorch` (una libreria de deep learning), `pipeline` (una libreria de tratamiento de datos), `scikit-learn` (una libreria de machine learning) y `transformers` (una libreria de modelos de lenguaje)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9987ceaeb8f560be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:06:47.825392Z",
     "start_time": "2024-01-17T21:06:09.205588Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: datasets in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: scikit-learn in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: transformers in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (3.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: packaging in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/carles/Documentos/notebooks/.venv/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las librerías necesarias en las versiones correctas\n",
    "\n",
    "%pip install --upgrade torch datasets scikit-learn transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6b60a6618dbd12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:07:05.113346Z",
     "start_time": "2024-01-17T21:06:47.834079Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Cargamos el conjunto de datos.Se descargará y almacenará automáticamente en local.\n",
    "# Este conjunto de datos contiene noticias de diferentes categorías.En este caso\n",
    "# Usaremos las categorías de mundo, deportes, negocios y ciencia ficción/tecnología.\n",
    "\n",
    "dataset = load_dataset('ag_news')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28e10ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['World', 'Sports', 'Business', 'Sci/Tech'], id=None)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['World', 'Sports', 'Business', 'Sci/Tech']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "\n",
    "print(dataset['train'].features)\n",
    "\n",
    "classes = dataset['train'].features[\"label\"].names\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba463cc1c41c1298",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Automáticamente, la función `load` ha dividido el conjunto de datos en dos conjuntos: un tren y una prueba.Para acceder a estos conjuntos, usaremos los atributos `train` i `test` del objeto `dataset`. Estos atributos son objetos `tf.data.Dataset` que contiene los ejemplos y etiquetas del conjunto de capacitación y prueba.Para acceder a ejemplos y etiquetas, utilizaremos los atributos `data` y `label` del objeto `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ca34052cf84fbd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:07:05.194181Z",
     "start_time": "2024-01-17T21:07:05.122941Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'exemples de train: 120000\n",
      "Nombre d'exemples de test: 7600\n"
     ]
    }
   ],
   "source": [
    "# Separar el conjunto de datos en capacitación y potencial\n",
    "ds_train = dataset['train']\n",
    "ds_test = dataset['test']\n",
    "# Veamos cuántos ejemplos hay en cada set\n",
    "print('Número de ejemplos de train:', len(ds_train))\n",
    "print('Número de ejemplos de test:', len(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3645de832fbdacf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Imprimimos los primeros 5 ejemplos del conjunto de entrenamiento.Como podemos ver, cada ejemplo es una noticia y su etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60e7bededaa593f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:07:05.480814Z",
     "start_time": "2024-01-17T21:07:05.152188Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 (Business) -> Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\n",
      "2 (Business) -> Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n",
      "2 (Business) -> Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n",
      "2 (Business) -> Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
      "2 (Business) -> Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n"
     ]
    }
   ],
   "source": [
    "# Imprimimos los primeros 5 ejemplos del conjunto de entrenamiento\n",
    "for w in ds_train.take(5):\n",
    "    print(f\"{w['label']} ({classes[w['label']]}) -> {w['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e77e47",
   "metadata": {},
   "source": [
    "## Tokenización\n",
    "\n",
    "La representación del texto en un modelo de idioma requiere que el texto se convierta en números. Si queremos una representación de nivel de palabra, necesitamos hacer dos cosas:\n",
    "* Utilizar un **tokenizador** para dividir el texto en **tokens**.\n",
    "* Construir un **vocabulario** con estos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54f8c05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['-', '-', 'hello', ',', 'how', 'are', 'you', 'doing', 'today', '?']\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "# Utilizamos el tokenizador de Bert (uno de los primeros modelos de lenguaje basados ​​en transformación) para tokenizar las oraciones\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "print(tokenizer.tokenize(\"-- Hello, how are you doing today?\"))\n",
    "\n",
    "# Podiamos ver el vocabulario de tokenización\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6424e",
   "metadata": {},
   "source": [
    "Usando el tokenizer, también podemos convertir nuestra cadena tokenizada en un conjunto de números:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "169e003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1011, 1011, 7592, 1010, 2129, 2024, 2017, 2725, 2651, 1029]\n"
     ]
    }
   ],
   "source": [
    "tokenitzada = tokenizer.tokenize(\"-- Hello, how are you doing today?\")\n",
    "\n",
    "def encode(text):\n",
    "    tk = tokenizer.tokenize(text)\n",
    "    return tokenizer.convert_tokens_to_ids(tk)\n",
    "\n",
    "print(encode(\"-- Hello, how are you doing today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8990e6d999d35898",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representación del texto\n",
    "\n",
    "Para entrenar un modelo de redes neuronales, necesitamos representar el texto como números. En esta práctica, usaremos la representación de la representación de la bolsa de palabras (BoW) que consiste en representar cada palabra como un número.Esta representación es muy simple y no tiene en cuenta el orden de las palabras o su semántica. Pero es una representación que funciona lo suficientemente bien en muchos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89259e6a26a7e3ac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representación de la bolsa de las palabras\n",
    "\n",
    "Aunque el significado de las palabras no es fácil de deducir sin poder acceder al contexto, en algunos casos, la representación de la bolsa de palabras puede ser útil.Por ejemplo, en el texto de una noticia, la palabra `covid` puede ser un buen indicador de que las noticias hablan sobre Covid-19 y la palabra `snow` puede ser un buen indicador de que las noticias hablan sobre el tiempo atmosférico.\n",
    "\n",
    "De las técnicas clásicas de vectorización de texto, la más simple es la representación de la bolsa de las palabras (BoW).En esta representación, cada palabra se representa como un número. Para convertir un texto en una representación de BoW, primero creamos un vector con tantos ceros como las palabras están en el vocabulario. Luego, para cada palabra del texto, aumentamos el valor de la posición correspondiente al vector por 1. Por ejemplo, si el texto es `this sentence is a test sentence`, el vector resultante seria `[1, 2, 1, 1, 0, 0, 0, 0, 0, 0, ...]`.\n",
    "\n",
    "Si recordamos la representación one-hot, veremos que la representación de BoW es muy similar. La diferencia es que la representación one-hot será una serie de vectores con un solo 1 y el resto de los valores en 0. En cambio, la representación de BoW será un vector con tantas veces que aparezca cada palabra. Podemos considerar que la representación de BoW sería la suma de vectores únicos.\n",
    "\n",
    "Por ejemplo, si el texto es `this sentence is a test sentence`, el vector one-hot de la primera palabra seria `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]` y el vector one-hot de la segunda palabra seria `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...]`. La representación de BoW sería la suma de estos dos vectores: `[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...]`.\n",
    "\n",
    "Para generar una representación de BoW, usaremos esta técnica para convertir cada palabra en un vector único y luego agregar todos los vectores.Para hacer esto, usaremos la función `to_bow` que crearemos a continuación. Esta función recibe un texto y devuelve un vector con la representación BoW del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d57f10bba32951f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:07:07.472087Z",
     "start_time": "2024-01-17T21:07:07.133342Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 2, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = [\n",
    "        'I like hot dogs.',\n",
    "        'The dog ran fast.',\n",
    "        'Its hot outside.',\n",
    "    ]\n",
    "vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.transform(['My dog likes hot dogs on a hot day.']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616ad0ef",
   "metadata": {},
   "source": [
    "Para calcular el vector BoW de una noticia de nuestro dataset AG_NEWS, podemos usar la siguiente función:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7990a193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "len_vocab = len(vocab)\n",
    "\n",
    "def to_bow(text, tamany_vocabulari=len_vocab):\n",
    "    res = torch.zeros(tamany_vocabulari, dtype=torch.float32)\n",
    "\n",
    "    for i in encode(text):\n",
    "        if i<tamany_vocabulari:\n",
    "            res[i] += 1\n",
    "    return res\n",
    "\n",
    "print(ds_train[0])\n",
    "print(to_bow(ds_train[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83bb2a026a54a15",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Entrenamiento de modelos de clasificación BoW\n",
    "\n",
    "Nuestro primer modelo será un clasificador de noticias utilizando la representación de BoW. Para hacer esto, crearemos un modelo de redes neuronales con una capa de entrada con tantas neuronas como las palabras están en nuestro vocabulario y una capa de salida con tantas neuronas como categorías que hay en nuestro conjunto de datos.\n",
    "\n",
    "#### Representación BoW\n",
    "\n",
    "Primero necesitamos convertir el texto en la representación de BoW utilizando la función `to_bow` Que hemos creado antes. Esta función recibe un texto y devuelve un vector con la representación BoW del texto.\n",
    "\n",
    "En pytorch se utilizan los `DataLoaders`, Para cargar los datos en lotes y convertirlos en tensores de Pytorch. Aprovecharemos esta funcionalidad para convertir los datos de BoW en tensores de PyTorch, Usando el parámetro `collate_fn` del `DataLoader` y proporcionando una función que convierta los datos textuales en tensores de BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fd6b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def bowify(batch):\n",
    "    ''' \n",
    "    Esta característica recibe una lista de noticias y devuelve un tensor con las etiquetas\n",
    "    (vector de floats) y otro con las noticias codificadas como BoW (matriz de floats donde cada fila\n",
    "    es un vector de BoW).\n",
    "    '''\n",
    "\n",
    "    # Las etiquetas son 0, 1, 2 o 3. \n",
    "    # Usamos Longtensor porque son enteros.\n",
    "\n",
    "    etiquetes = torch.LongTensor([noticia[\"label\"] for noticia in batch])\n",
    "\n",
    "    # La noticias son tensores de BoW\n",
    "    noticies = torch.stack([to_bow(noticia[\"text\"]) for noticia in batch])\n",
    "\n",
    "    return (\n",
    "            etiquetes,\n",
    "            noticies\n",
    "    )\n",
    "\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=16, collate_fn=bowify)\n",
    "test_loader = DataLoader(ds_test, batch_size=16, collate_fn=bowify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85d6ad8ccb956f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Modelo de classificación\n",
    "\n",
    "Ahora definamos una red neuronal clasificadora simple que contiene una capa lineal. El tamaño del vector de entrada es igual a `vocab_size`, y el tamaño de salida corresponde al número de clases (4). Debido a que estamos resolviendo la tarea de clasificación, la función de activación final es `LogSoftmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f785d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(len(vocab), 4),\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d5e68",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo\n",
    "\n",
    "Ahora definiremos el bucle de entrenamiento estándar de Pytorch. Debido a que nuestro conjunto de datos es bastante grande, con nuestro propósito de enseñanza, entrenaremos solo para una epoca, y a veces incluso por menos de una epoca (especificando el parámetro `epoch_size` nos permite limitar el entrenamiento). También informaremos la precisión del entrenamiento acumulado durante el entrenamiento; La frecuencia de notificación se especifica utilizando el parámetro `report_freq`.\n",
    "\n",
    "Para entrenar el modelo, usaremos el optimizador `Adam`(ya que es uno de los optimizadores más utilizados) y la función de costo `CrossEntropyLoss` (ya que tenemos un problema de calificación con más de dos clases). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e46fdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.8690625\n",
      "6400: acc=0.88875\n",
      "9600: acc=0.8964583333333334\n",
      "12800: acc=0.90390625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026751213236404125, 0.9080490405117271)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_epoch(\n",
    "    net,\n",
    "    dataloader,\n",
    "    lr=0.01,\n",
    "    optimizer=None,\n",
    "    loss_fn=torch.nn.NLLLoss(),\n",
    "    epoch_size=None,\n",
    "    report_freq=200,\n",
    "):\n",
    "    # Si no se especifica un optimizador, usamos Adam\n",
    "    optimizer = optimizer or torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    # Ponemos la red en modo de entrenamiento.Esto activa el comportamiento de las capas de DropOut, por ejemplo.\n",
    "    net.train()\n",
    "\n",
    "    # Inicializar las variables que nos servirán para calcular la precisión\n",
    "    total_loss, acc, count, i = 0, 0, 0, 0\n",
    "\n",
    "    # Iteremamos sobre el dataloader\n",
    "    for labels, features in dataloader:\n",
    "\n",
    "        # Ponemos los gradientes a cero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculamos la salida de la red\n",
    "        out = net(features)\n",
    "\n",
    "        # Calculamos la pérdida. Esta función ya se aplica a Softmax a la salida.\n",
    "        loss = loss_fn(out, labels)  # cross_entropy(out,labels)\n",
    "\n",
    "        # Propagamos la pérdida de regreso. Esto hará que se calculen los gradientes .\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los pesos de la red. Esto toma un paso de optimización.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos variables para calcular la precisión.\n",
    "        total_loss += loss\n",
    "\n",
    "        # Calculamos la precisión. Para hacer esto, debemos convertir la salida de red en etiquetas.\n",
    "        # La clase con la mayor probabilidad es la que predecimos como etiqueta.\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        acc += (predicted == labels).sum()\n",
    "\n",
    "        # Actualizamos el contador de muestras\n",
    "        count += len(labels)\n",
    "\n",
    "        # Mostramos la precisión cada report_freq muestras\n",
    "        i += 1\n",
    "        if i % report_freq == 0:\n",
    "            print(f\"{count}: acc={acc.item()/count}\")\n",
    "\n",
    "        # Si se especifica epoch_size y ya hemos procesado este número de muestras, dejamos el bucle.\n",
    "        if epoch_size and count > epoch_size:\n",
    "            break\n",
    "    return total_loss.item() / count, acc.item() / count\n",
    "\n",
    "\n",
    "train_epoch(net, train_loader, epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3230bbc2532904",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "El modelo ha logrado una precisión de más de `0.9` en el conjunto de entrenamiento; Un número suficientemente aceptable considerando que hemos simplificado el problema para reducir el tiempo de ejecución del tutorial. En un caso real, usaríamos todas las noticias del conjunto de entrenamiento y el modelo sería más preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe580202e44097",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representación de Word2Vec\n",
    "\n",
    "La representación de Word2Vec es una representación ampliamente utilizada en el procesamiento del lenguaje natural. Esta representación tiene en cuenta el contexto de las palabras y permite operaciones con las palabras. Por ejemplo, si restamos la palabra vector `king` y sumamos el vector de la palabra `woman`, obtendremos un vector que será muy similar al vector de la plabra `queen`.\n",
    "\n",
    "Para generar representación de Word2Vec, usaremos la librería `gensim`. Esta librería contiene muchos modelos de representación de palabras. En este caso usaremos el modelo `word2vec-google-news-300` que contiene la representación de Word2Vec de 3 millones de palabras y frases. \n",
    "\n",
    "> La primera vez que esta celda se está ejecutando, la función `load` Descargará el modelo de 1.5GB.Esto puede tomar unos minutos. Una vez descargado, el modelo se almacenará en la carpeta `/home/USUARI/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz` Y no necesitará descargarlo nuevamente.\n",
    "> Esta función devuelve un objeto `KeyedVectors` que contiene la representación Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12929ab110408820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:14:40.504566Z",
     "start_time": "2024-01-17T21:12:31.589189Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9fa1231614bce",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ahora podemos acceder a la representación de la palabra 2VEC de cada palabra.Por ejemplo, para acceder a la representación de la palabra `king`, Usaremos la función `get_vector` del objeto `KeyedVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fee703eb5efc219a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:14:41.336216Z",
     "start_time": "2024-01-17T21:14:40.524698Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25976562e-01,  2.97851562e-02,  8.60595703e-03,  1.39648438e-01,\n",
       "       -2.56347656e-02, -3.61328125e-02,  1.11816406e-01, -1.98242188e-01,\n",
       "        5.12695312e-02,  3.63281250e-01, -2.42187500e-01, -3.02734375e-01,\n",
       "       -1.77734375e-01, -2.49023438e-02, -1.67968750e-01, -1.69921875e-01,\n",
       "        3.46679688e-02,  5.21850586e-03,  4.63867188e-02,  1.28906250e-01,\n",
       "        1.36718750e-01,  1.12792969e-01,  5.95703125e-02,  1.36718750e-01,\n",
       "        1.01074219e-01, -1.76757812e-01, -2.51953125e-01,  5.98144531e-02,\n",
       "        3.41796875e-01, -3.11279297e-02,  1.04492188e-01,  6.17675781e-02,\n",
       "        1.24511719e-01,  4.00390625e-01, -3.22265625e-01,  8.39843750e-02,\n",
       "        3.90625000e-02,  5.85937500e-03,  7.03125000e-02,  1.72851562e-01,\n",
       "        1.38671875e-01, -2.31445312e-01,  2.83203125e-01,  1.42578125e-01,\n",
       "        3.41796875e-01, -2.39257812e-02, -1.09863281e-01,  3.32031250e-02,\n",
       "       -5.46875000e-02,  1.53198242e-02, -1.62109375e-01,  1.58203125e-01,\n",
       "       -2.59765625e-01,  2.01416016e-02, -1.63085938e-01,  1.35803223e-03,\n",
       "       -1.44531250e-01, -5.68847656e-02,  4.29687500e-02, -2.46582031e-02,\n",
       "        1.85546875e-01,  4.47265625e-01,  9.58251953e-03,  1.31835938e-01,\n",
       "        9.86328125e-02, -1.85546875e-01, -1.00097656e-01, -1.33789062e-01,\n",
       "       -1.25000000e-01,  2.83203125e-01,  1.23046875e-01,  5.32226562e-02,\n",
       "       -1.77734375e-01,  8.59375000e-02, -2.18505859e-02,  2.05078125e-02,\n",
       "       -1.39648438e-01,  2.51464844e-02,  1.38671875e-01, -1.05468750e-01,\n",
       "        1.38671875e-01,  8.88671875e-02, -7.51953125e-02, -2.13623047e-02,\n",
       "        1.72851562e-01,  4.63867188e-02, -2.65625000e-01,  8.91113281e-03,\n",
       "        1.49414062e-01,  3.78417969e-02,  2.38281250e-01, -1.24511719e-01,\n",
       "       -2.17773438e-01, -1.81640625e-01,  2.97851562e-02,  5.71289062e-02,\n",
       "       -2.89306641e-02,  1.24511719e-02,  9.66796875e-02, -2.31445312e-01,\n",
       "        5.81054688e-02,  6.68945312e-02,  7.08007812e-02, -3.08593750e-01,\n",
       "       -2.14843750e-01,  1.45507812e-01, -4.27734375e-01, -9.39941406e-03,\n",
       "        1.54296875e-01, -7.66601562e-02,  2.89062500e-01,  2.77343750e-01,\n",
       "       -4.86373901e-04, -1.36718750e-01,  3.24218750e-01, -2.46093750e-01,\n",
       "       -3.03649902e-03, -2.11914062e-01,  1.25000000e-01,  2.69531250e-01,\n",
       "        2.04101562e-01,  8.25195312e-02, -2.01171875e-01, -1.60156250e-01,\n",
       "       -3.78417969e-02, -1.20117188e-01,  1.15234375e-01, -4.10156250e-02,\n",
       "       -3.95507812e-02, -8.98437500e-02,  6.34765625e-03,  2.03125000e-01,\n",
       "        1.86523438e-01,  2.73437500e-01,  6.29882812e-02,  1.41601562e-01,\n",
       "       -9.81445312e-02,  1.38671875e-01,  1.82617188e-01,  1.73828125e-01,\n",
       "        1.73828125e-01, -2.37304688e-01,  1.78710938e-01,  6.34765625e-02,\n",
       "        2.36328125e-01, -2.08984375e-01,  8.74023438e-02, -1.66015625e-01,\n",
       "       -7.91015625e-02,  2.43164062e-01, -8.88671875e-02,  1.26953125e-01,\n",
       "       -2.16796875e-01, -1.73828125e-01, -3.59375000e-01, -8.25195312e-02,\n",
       "       -6.49414062e-02,  5.07812500e-02,  1.35742188e-01, -7.47070312e-02,\n",
       "       -1.64062500e-01,  1.15356445e-02,  4.45312500e-01, -2.15820312e-01,\n",
       "       -1.11328125e-01, -1.92382812e-01,  1.70898438e-01, -1.25000000e-01,\n",
       "        2.65502930e-03,  1.92382812e-01, -1.74804688e-01,  1.39648438e-01,\n",
       "        2.92968750e-01,  1.13281250e-01,  5.95703125e-02, -6.39648438e-02,\n",
       "        9.96093750e-02, -2.72216797e-02,  1.96533203e-02,  4.27246094e-02,\n",
       "       -2.46093750e-01,  6.39648438e-02, -2.25585938e-01, -1.68945312e-01,\n",
       "        2.89916992e-03,  8.20312500e-02,  3.41796875e-01,  4.32128906e-02,\n",
       "        1.32812500e-01,  1.42578125e-01,  7.61718750e-02,  5.98144531e-02,\n",
       "       -1.19140625e-01,  2.74658203e-03, -6.29882812e-02, -2.72216797e-02,\n",
       "       -4.82177734e-03, -8.20312500e-02, -2.49023438e-02, -4.00390625e-01,\n",
       "       -1.06933594e-01,  4.24804688e-02,  7.76367188e-02, -1.16699219e-01,\n",
       "        7.37304688e-02, -9.22851562e-02,  1.07910156e-01,  1.58203125e-01,\n",
       "        4.24804688e-02,  1.26953125e-01,  3.61328125e-02,  2.67578125e-01,\n",
       "       -1.01074219e-01, -3.02734375e-01, -5.76171875e-02,  5.05371094e-02,\n",
       "        5.26428223e-04, -2.07031250e-01, -1.38671875e-01, -8.97216797e-03,\n",
       "       -2.78320312e-02, -1.41601562e-01,  2.07031250e-01, -1.58203125e-01,\n",
       "        1.27929688e-01,  1.49414062e-01, -2.24609375e-02, -8.44726562e-02,\n",
       "        1.22558594e-01,  2.15820312e-01, -2.13867188e-01, -3.12500000e-01,\n",
       "       -3.73046875e-01,  4.08935547e-03,  1.07421875e-01,  1.06933594e-01,\n",
       "        7.32421875e-02,  8.97216797e-03, -3.88183594e-02, -1.29882812e-01,\n",
       "        1.49414062e-01, -2.14843750e-01, -1.83868408e-03,  9.91210938e-02,\n",
       "        1.57226562e-01, -1.14257812e-01, -2.05078125e-01,  9.91210938e-02,\n",
       "        3.69140625e-01, -1.97265625e-01,  3.54003906e-02,  1.09375000e-01,\n",
       "        1.31835938e-01,  1.66992188e-01,  2.35351562e-01,  1.04980469e-01,\n",
       "       -4.96093750e-01, -1.64062500e-01, -1.56250000e-01, -5.22460938e-02,\n",
       "        1.03027344e-01,  2.43164062e-01, -1.88476562e-01,  5.07812500e-02,\n",
       "       -9.37500000e-02, -6.68945312e-02,  2.27050781e-02,  7.61718750e-02,\n",
       "        2.89062500e-01,  3.10546875e-01, -5.37109375e-02,  2.28515625e-01,\n",
       "        2.51464844e-02,  6.78710938e-02, -1.21093750e-01, -2.15820312e-01,\n",
       "       -2.73437500e-01, -3.07617188e-02, -3.37890625e-01,  1.53320312e-01,\n",
       "        2.33398438e-01, -2.08007812e-01,  3.73046875e-01,  8.20312500e-02,\n",
       "        2.51953125e-01, -7.61718750e-02, -4.66308594e-02, -2.23388672e-02,\n",
       "        2.99072266e-02, -5.93261719e-02, -4.66918945e-03, -2.44140625e-01,\n",
       "       -2.09960938e-01, -2.87109375e-01, -4.54101562e-02, -1.77734375e-01,\n",
       "       -2.79296875e-01, -8.59375000e-02,  9.13085938e-02,  2.51953125e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.get_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1217873948a0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "También podemos acceder a las palabras más similares a una palabra. Por ejemplo, para acceder a las palabras más similares a la palabra `king`, usaremos la función `most_similar` del objeto `KeyedVectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bffdc3f5aef651db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:14:57.407515Z",
     "start_time": "2024-01-17T21:14:40.936113Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kings -> 0.7138046622276306\n",
      "queen -> 0.6510956287384033\n",
      "monarch -> 0.6413194537162781\n",
      "crown_prince -> 0.6204220056533813\n",
      "prince -> 0.6159993410110474\n",
      "sultan -> 0.5864824056625366\n",
      "ruler -> 0.5797567367553711\n",
      "princes -> 0.5646552443504333\n",
      "Prince_Paras -> 0.543294370174408\n",
      "throne -> 0.5422105193138123\n"
     ]
    }
   ],
   "source": [
    "for w, p in w2v.most_similar('king'):\n",
    "    print(f\"{w} -> {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa38ee95af3d11",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lo más interesante de la representación de Word2Vec es que los vectores tienen una estructura matemática que nos permite realizar operaciones con las palabras. Por ejemplo, si restamos el vector de la palabra `man` al vector de la palabra `king` y sumamos el vector de la palabra `woman`, obtendremos un vector que será muy similar al vector de la palabra `queen`.\n",
    "\n",
    "$$ KING - MAN + WOMAN = QUEEN $$\n",
    "\n",
    "Para hacer esta operación, usaremos la función `most_similar` del objeto `KeyedVectors` y pasaremos los vectores de las palabras `king`, `woman` y `man`. Esta característica devolverá una lista con las palabras más similares al vector resultante.Como podemos ver, la palabra más similar es `queen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f79029055681603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-17T21:14:57.817533Z",
     "start_time": "2024-01-17T21:14:57.418430Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('queen', 0.7118191719055176)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['king', 'woman'], negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a76b6e",
   "metadata": {},
   "source": [
    "### Clasificador de Word2Vec\n",
    "\n",
    "Ahora crearemos un clasificador de noticias utilizando la representación Word2Vec. Primero tendremos que obtener la representación de cada palabra para convertir el texto en vectores. Luego agregaremos todos los vectores para obtener un vector para cada noticia. Este vector será la representación de las noticias.\n",
    "\n",
    "Para convertir un texto en un vector, usaremos la función `to_w2v` que crearemos a continuación. Esta función recibe un texto y devuelve un vector con la representación Word2Vec del texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce374416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-17.0809,  11.0404,  -0.9337,  12.4042,  -6.2286,   3.0224, -10.0442,\n",
      "         -8.5156,  -5.9407,   1.1501,  -3.8471,  -8.0006, -18.2444,   4.3982,\n",
      "        -14.2061,  11.0110,  11.2352,  14.8521,  -2.5686,   2.8961, -22.3914,\n",
      "         -3.2182,   9.7872,   0.3238,  -8.6214,   4.2367, -21.9348,   5.7704,\n",
      "         -0.6942,  -1.7075,  -2.4800,   2.1805,  -7.0602, -12.3824, -11.6949,\n",
      "          8.2563, -18.9995,  11.3932,  -7.3198,   7.3370,  -6.1129,  -3.6244,\n",
      "          5.8519,   8.3060,   3.9137,  -1.8091,  -3.2730, -15.8203,  -9.6418,\n",
      "          8.9092, -16.8270,  24.5614,  -2.5387,  21.7112,   6.0571,  14.3324,\n",
      "        -17.4978, -12.2693,   1.1129, -15.9192, -12.1886,  -9.5650, -19.0873,\n",
      "         -7.7948,  -4.9111, -18.4653, -10.2332,  11.3437,  -6.0452,   5.4705,\n",
      "          3.7500,  -9.5068,   4.4747,  -0.2912,  -3.9221,   0.3543,  13.0927,\n",
      "          2.3088,   3.5300, -11.2126, -14.8031,  -2.9008,  -3.4219,  -0.3365,\n",
      "         13.8353,   7.0914,  -5.2219,  22.0132,   4.2657,   5.8488,  -0.5776,\n",
      "         -1.5022,  -5.0004, -13.3813,   4.8757,  10.3992,  -9.8992,  10.6411,\n",
      "         25.6584,  -3.4937,  -6.5989,  -1.0960,  -6.7775,   0.1842,  -6.0798,\n",
      "         13.2260,  -6.2332,   4.9711,   0.8566,   4.1002, -11.5986, -16.8590,\n",
      "         -6.4362,  -3.6979,   4.9203,  15.2933,   7.6364,  -5.8566,  -1.6903,\n",
      "         -2.3312,  12.3486,   7.5709,  -0.6597,   2.7831,  12.6196, -15.9392,\n",
      "         -9.4420,  -1.7229,   7.7839,  10.5602,  -5.9280,  -2.6489,  -6.4361,\n",
      "         -3.8383, -16.0124,   8.0287,  -3.4375,   2.8186,  22.9197,  13.0072,\n",
      "         20.2472,  -6.0054,   4.0575,  -4.7046,  -6.8406, -12.1006,  -4.0645,\n",
      "         18.0959,  -4.8794,   1.5283,   8.0677, -28.4229,  -6.3982,  -4.6095,\n",
      "         -8.2329,  -7.8615,   8.5930,  14.3553,  -2.6136,  -0.7672,   0.5814,\n",
      "          6.9687,   0.7667,   0.1969,  -1.1499,  -4.6281,  15.1071,   5.1883,\n",
      "        -10.1836,   6.9755,  -9.1791,  -8.8102,  -6.4574,  -8.9768,  -1.3514,\n",
      "         20.5255,  22.8086, -22.4160,  -2.1751,  -5.2745,   0.4971,   1.9747,\n",
      "          5.5252,  -4.8856,  -2.1867,   5.9344,  -5.4659,   5.1147,  -3.3837,\n",
      "          5.4895,  12.1746,  -4.1896, -27.1298,  -4.4509,  10.7126,   4.8896,\n",
      "         -6.0110,  -0.7719,   7.8879, -10.4668,  -9.2913,   2.1059, -19.3102,\n",
      "        -10.8101,   5.4989,  -7.4446,  -4.7968,   9.8521,  -3.9826,  14.8542,\n",
      "         16.3674,   7.4929, -11.3996,   1.8357,  -8.1945,   6.2330,  15.2261,\n",
      "         -3.4122, -16.1802,  -2.0000, -12.0552,  11.2962,   5.6537,  -0.8348,\n",
      "         -0.8463,  -6.4080,   5.8111,   2.4668,   1.0925, -14.5064,   1.1021,\n",
      "         -4.3229,  -8.5156,   1.3596,   0.2417,   1.4028,   7.4663,   8.9206,\n",
      "          7.3249,   8.0591,   7.7924,   6.9987,  27.3159,  -4.7353,   0.7053,\n",
      "          6.7754, -12.8845,  13.8699,   5.8623,  -6.8129,   5.8627,   2.7595,\n",
      "          6.3065,   9.9255,  -2.8854, -10.1693,  -7.0736,   7.9216,   2.5093,\n",
      "         -9.5866,   7.4031,  -0.9011,   9.9832,  -2.0049,  -6.3317,   0.4062,\n",
      "          0.0936,   1.1288,  -2.5539,  -2.7307,  -5.4014,  -2.8721,   1.3374,\n",
      "          0.2924,   3.5125,  -8.9189, -15.5585, -13.4326,  -9.4054,   3.7766,\n",
      "        -12.4168,  12.1424,  -1.4987,   0.1738,  -0.9734,   6.0570,  -0.8122,\n",
      "         -3.2520,  -5.7413,  -4.4579,   4.8879,  -0.8176,  -9.8232,   8.6069,\n",
      "         -3.3508, -15.2089, -10.0510,  -1.9859, -10.7878,  18.1031])\n"
     ]
    }
   ],
   "source": [
    "def to_w2v(text):\n",
    "    res = torch.zeros(300, dtype=torch.float32)\n",
    "    for word in text:\n",
    "        if word in w2v:\n",
    "            res += torch.tensor(w2v.get_vector(word))\n",
    "    return res\n",
    "\n",
    "print(to_w2v(ds_train[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b76ac",
   "metadata": {},
   "source": [
    "Como lo hicimos con la representación de BoW, usaremos el `DataLoaders` de PyTorch para convertir los datos en vectores Word2Vec en tensores Pytorch. Aprovecharemos el parámetro `collate_fn` del `DataLoader` para proporcionar una función que convierta los datos textuales en tensores Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a6c88fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2vify(batch):\n",
    "    etiquetes = torch.LongTensor([noticia[\"label\"] for noticia in batch])\n",
    "    noticies = torch.stack([to_w2v(tokenizer.tokenize(noticia[\"text\"])) for noticia in batch])\n",
    "    return etiquetes, noticies\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=16, collate_fn=w2vify)\n",
    "test_loader = DataLoader(ds_test, batch_size=16, collate_fn=w2vify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dffc153",
   "metadata": {},
   "source": [
    "### Modelo de clasificación\n",
    "\n",
    "Ahora crearemos el modelo de clasificación usando Pytorch. Definiremos un modelo simple con una capa lineal. El tamaño del vector de entrada será `300` (el tamaño de la representación Word2Vec) y el tamaño de la salida será el número de clases (4). Como estamos resolviendo una tarea de clasificación, la función de activación final será `LogSoftmax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd0fd6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(300, 4),\n",
    "    torch.nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649418d1",
   "metadata": {},
   "source": [
    "Finalmente, entrenamos al modelo utilizando el mismo procedimiento que hemos realizado con la representación de BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55612930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200: acc=0.793125\n",
      "6400: acc=0.80671875\n",
      "9600: acc=0.8083333333333333\n",
      "12800: acc=0.81375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09231998468004564, 0.8166311300639659)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoch(net, train_loader, epoch_size=15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754f4cd2c374fd4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "El resultado no es muy bueno. Esto se debe a que el modelo Word2Vec que utilizamos no tiene las palabras que aparecen en el conjunto de datos. Por ejemplo, si buscamos la palabra `covid`, veremos que no aparece en el modelo.\n",
    "\n",
    "Para resolver este problema, tendremos que usar un modelo Word2Vec entrenado con las palabras del conjunto de datos. Pero esto es muy lento y no lo haremos en este tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
