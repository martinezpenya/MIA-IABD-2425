{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6fed506-8969-48cd-9f67-e7f52bc33217",
      "metadata": {
        "id": "f6fed506-8969-48cd-9f67-e7f52bc33217"
      },
      "source": [
        "# Primeros pasos con SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abc46bce-e563-402f-b2e1-97876f433ee0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abc46bce-e563-402f-b2e1-97876f433ee0",
        "outputId": "13de6997-51b5-414e-fa82-0f2cfd19598f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "[('Esto', 'PRON'), ('es', 'AUX'), ('una', 'DET'), ('frase', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "#importamos todos los módulos necesarios\n",
        "import spacy\n",
        "import spacy.cli\n",
        "#descargamos el modelo pre-entrenado de español (también hay catalan), y tienen diferentes longitudes y precisiones\n",
        "\n",
        "#este modelo es el más grande y más preciso en español\n",
        "#spacy.cli.download(\"es_dep_news_trf\")\n",
        "#nlp = spacy.load(\"es_dep_news_trf\")\n",
        "\n",
        "#tenemos también es_core_news_sm (español más rápido, pequeño y menos preciso)\n",
        "spacy.cli.download(\"es_core_news_sm\")\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "#o ca_core_news_sm y ca_core_news_trf (catalan)\n",
        "\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n",
        "#lo probamos con una frase\n",
        "doc = nlp(\"Esto es una frase.\")\n",
        "print([(w.text, w.pos_) for w in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be3299a-6d05-4569-9b27-80e12dc54120",
      "metadata": {
        "id": "7be3299a-6d05-4569-9b27-80e12dc54120"
      },
      "source": [
        "## Preprocesamiento de datos\n",
        "\n",
        "El procesamiento de datos en spaCy es ligeramente más sencillo en spaCy que en NLTK. En este caso se deben importar los modelos entrenados que se han descargado antes y usarlos para obtener los tokens ya tematizados y en minúsculas. Proceso que se puede ver en él siguiente código."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eed073f-cb03-4819-a84e-ff8519d692d6",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5eed073f-cb03-4819-a84e-ff8519d692d6",
        "outputId": "b5c52771-4d40-4d32-9d7e-0ca07c1f9c43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'encantar contenido blog analytics lane artículo fantástico'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "text = \"Me encanta el contenido del blog de Analytics Lane, los artículos son fantásticos.\"\n",
        "\n",
        "#nlp = spacy.load('es_core_news_sm')\n",
        "doc = nlp(text)\n",
        "\n",
        "# Eliminación de palabras irrelevantes (stopwords) y signos de puntuación\n",
        "tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Reconstrucción del texto preprocesado\n",
        "preprocessed_text = ' '.join(tokens)\n",
        "preprocessed_text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f4dcaa7-dddf-4e96-9ccd-00a7c12cefc0",
      "metadata": {
        "id": "5f4dcaa7-dddf-4e96-9ccd-00a7c12cefc0"
      },
      "source": [
        "En este se cargan los modelos mediante la función `spacy.load()` (si se ha descargado otro se deberá reemplazar el nombre en el parámetro de la función por el que se desee usar). Al modelo importado se le puede pasar la cadena de texto para obtener los tokens (`nlp(text)`). Posteriormente se filtran aquellos que no son ni stopwords ni elementos de puntuación (not token.is_stop and not token.is_punct) para, en el mismo paso, lematizar y convertir en minúsculas (`token.lemma_.lower()`). El resultado es el listado de tokens procesados.\n",
        "\n",
        "Nótese como la herramienta usa como token el infinitivo del verbo en lugar de su versión conjugada (encantar en lugar de encanta). También los términos en plural aparecen en singular (artículo en lugar de artículos). Lo que facilita el análisis de los textos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a6ae2e-5358-4c6f-82bd-73a0a693602b",
      "metadata": {
        "id": "92a6ae2e-5358-4c6f-82bd-73a0a693602b"
      },
      "source": [
        "## Extracción de características\n",
        "En spaCy, la extracción de características también se puede hacer de una forma sencilla. Para lo que se debe iterar sobre los tokens y crear una lista con el conteo de términos. Una posible opción para hacer esto es la que se muestra en el siguiente código de ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dea9c20-63c4-408c-ad47-01c481e44ef6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dea9c20-63c4-408c-ad47-01c481e44ef6",
        "outputId": "ee44e97f-8109-49e7-a801-5ef141b6e36e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'encantar': 1,\n",
              " 'contenido': 1,\n",
              " 'blog': 1,\n",
              " 'analytics': 1,\n",
              " 'lane': 1,\n",
              " 'artículo': 1,\n",
              " 'fantástico': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "features = {}\n",
        "doc = nlp(preprocessed_text)\n",
        "\n",
        "for token in doc:\n",
        "    if not token.is_stop and not token.is_punct:\n",
        "        if token.lemma_.lower() in features:\n",
        "            features[token.lemma_.lower()] += 1\n",
        "        else:\n",
        "            features[token.lemma_.lower()] = 1\n",
        "\n",
        "features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23df77c0-2be4-4c69-805f-e872535cb8b5",
      "metadata": {
        "id": "23df77c0-2be4-4c69-805f-e872535cb8b5"
      },
      "source": [
        "Lo que genera un diccionario donde la palabra es la clave y el valor es el número de ocurrencias de esta en el texto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d36e5a81-71d8-4d89-b370-82a1fe7307fa",
      "metadata": {
        "id": "d36e5a81-71d8-4d89-b370-82a1fe7307fa"
      },
      "source": [
        "## Conjunto de datos de entrenamiento y factorización de los datos\n",
        "Ahora, antes de poder hacer un análisis de sentimientos en español con spaCy es necesario disponer de un conjunto de datos para el entrenamiento. Para lo que se recurre a una traducción del conjunto que se ha utilizado la semana pasada con NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23786ddc-b9c8-4cfc-95dd-b7a868efa66a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23786ddc-b9c8-4cfc-95dd-b7a868efa66a",
        "outputId": "1d5e5cf7-1c30-45a2-feab-67b2d117a7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Me encanta el contenido del blog de Analytics Lane, los artículos son fantásticos.', 'positivo'), ('El código no funciona, me ha dado un error al ejecutarlo.', 'negativo'), ('Me encanta este producto.', 'positivo'), ('Esta película fue terrible.', 'negativo'), ('El clima está agradable hoy.', 'positivo'), ('Me siento triste por las noticias.', 'negativo'), ('Es solo un libro promedio.', 'neutral')]\n"
          ]
        }
      ],
      "source": [
        "training_data = [\n",
        "    (\"Me encanta el contenido del blog de Analytics Lane, los artículos son fantásticos.\", \"positivo\"),\n",
        "    (\"El código no funciona, me ha dado un error al ejecutarlo.\", \"negativo\"),\n",
        "    (\"Me encanta este producto.\", \"positivo\"),\n",
        "    (\"Esta película fue terrible.\", \"negativo\"),\n",
        "    (\"El clima está agradable hoy.\", \"positivo\"),\n",
        "    (\"Me siento triste por las noticias.\", \"negativo\"),\n",
        "    (\"Es solo un libro promedio.\", \"neutral\")\n",
        "]\n",
        "print(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d03fc1-d0f9-4f8b-b26a-06c53745b981",
      "metadata": {
        "id": "13d03fc1-d0f9-4f8b-b26a-06c53745b981"
      },
      "source": [
        "Otra cosa que también se puede hacer es crear funciones con las que se factorizan los pasos vistos en las secciones anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420a3bc6-3599-4508-a921-0d294c73fb1f",
      "metadata": {
        "id": "420a3bc6-3599-4508-a921-0d294c73fb1f"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Realiza el preprocesamiento básico de un texto en idioma español utilizando spaCy.\n",
        "\n",
        "    Args:\n",
        "        text (str): El texto a ser preprocesado.\n",
        "\n",
        "    Returns:\n",
        "        str: El texto preprocesado.\n",
        "    \"\"\"\n",
        "    nlp = spacy.load('es_core_news_sm')\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Eliminación de palabras irrelevantes (stopwords) y signos de puntuación\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "    # Reconstrucción del texto preprocesado\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "def extract_features(text):\n",
        "    \"\"\"\n",
        "    Extrae las características del texto utilizando spaCy y devuelve un diccionario de características.\n",
        "\n",
        "    Args:\n",
        "        text (str): El texto del cual extraer características.\n",
        "\n",
        "    Returns:\n",
        "        dict: Un diccionario que representa las características extraídas del texto.\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        if not token.is_stop and not token.is_punct:\n",
        "            if token.lemma_.lower() in features:\n",
        "                features[token.lemma_.lower()] += 1\n",
        "            else:\n",
        "                features[token.lemma_.lower()] = 1\n",
        "    return features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1464bc1-8677-4289-8f62-296c8224ea12",
      "metadata": {
        "id": "a1464bc1-8677-4289-8f62-296c8224ea12"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "Para el análisis de sentimiento uno de los modelos que mejor funciona son `Naive Bayes`. A diferencia de NLTK, spaCy no cuenta con una implementación propia, pero se puede recurrir a la que existe en `Scikit-learn`. Así, para entrenar el modelo solamente sería necesario preprocesar los datos, extraer las características y crear un conjunto de entrenamiento para el modelo `MultinomialNB()`. Pasos que se pueden implementar como se muestra a continuación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97dc96ba-cb7b-4746-81fb-3cd4d89c1344",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97dc96ba-cb7b-4746-81fb-3cd4d89c1344",
        "outputId": "b190d34f-e8e0-4f94-e7e9-826733ee7b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('encantar contenido blog analytics lane artículo fantástico', 'positivo'), ('código funcionar error ejecutar él', 'negativo'), ('encantar producto', 'positivo'), ('película terrible', 'negativo'), ('clima agradable', 'positivo'), ('sentir triste noticia', 'negativo'), ('libro promedio', 'neutral')]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Preprocesamiento de los datos de entrenamiento\n",
        "preprocessed_training_data = [(preprocess_text(text), label) for text, label in training_data]\n",
        "print(preprocessed_training_data)\n",
        "# Extracción de características de los datos de entrenamiento\n",
        "training_features = [extract_features(text) for text, _ in preprocessed_training_data]\n",
        "vectorizer = DictVectorizer(sparse=False)\n",
        "X_train = vectorizer.fit_transform(training_features)\n",
        "\n",
        "# Etiquetas de los datos de entrenamiento\n",
        "y_train = [label for _, label in preprocessed_training_data]\n",
        "\n",
        "# Entrenamiento del clasificador Naive Bayes\n",
        "classifier = MultinomialNB()\n",
        "_ = classifier.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c2c488f-08d5-4c06-8612-7229c0781a9b",
      "metadata": {
        "id": "5c2c488f-08d5-4c06-8612-7229c0781a9b"
      },
      "source": [
        "## Clasificación de nuevos textos\n",
        "\n",
        "Ahora, una vez entrenado el modelo, se puede usar esta para predecir el sentimiento de los nuevos textos. Simplemente se repiten con los nuevos textos las transformaciones realizadas sobre el conjunto de entrenamiento y el resultado se le pasa al modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86311334-4010-4dc8-821f-a422b2bdb15c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86311334-4010-4dc8-821f-a422b2bdb15c",
        "outputId": "9f099a2e-e358-4a01-b6cb-777d65a6242b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentimiento: positivo\n"
          ]
        }
      ],
      "source": [
        "# Nuevo texto para clasificar\n",
        "new_text = \"Me encantó mucho del concierto.\"\n",
        "\n",
        "# Preprocesamiento del nuevo texto\n",
        "preprocessed_text = preprocess_text(new_text)\n",
        "\n",
        "# Extracción de características del nuevo texto\n",
        "features = extract_features(preprocessed_text)\n",
        "X_test = vectorizer.transform([features])\n",
        "\n",
        "# Clasificación del nuevo texto\n",
        "sentiment = classifier.predict(X_test)\n",
        "print(\"Sentimiento:\", sentiment[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9285af19-c006-4a91-8832-9588fea8934d",
      "metadata": {
        "id": "9285af19-c006-4a91-8832-9588fea8934d"
      },
      "source": [
        "## Conclusiones\n",
        "`spaCy` es una librería alternativa `NLTK` con la que también se puede realizar análisis de sentimientos. Contando con la ventaja de que también se puede hacer en español y otros idiomas gracias a los modelos pre-entrenados que se pueden descargar. Algo que también simplifica el trabajo con la librería. Por eso, en el caso de querer realizar análisis de sentimientos en español, `spaCy` es una de las opciones que debe tener siempre en cuenta."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1146781b-733f-4341-84a4-dc6d5db09568",
      "metadata": {
        "id": "1146781b-733f-4341-84a4-dc6d5db09568"
      },
      "source": [
        "## Fuentes de información\n",
        "* https://spacy.io/models/\n",
        "* https://www.analyticslane.com/2023/05/29/analisis-de-sentimientos-en-espanol-con-spacy-en-python/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}